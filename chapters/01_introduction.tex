% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

% \thechapter{Introduction}\label{chapter:introduction}


% Citation test~\parencite{latex}.

% Acronyms must be added in \texttt{main.tex} and are referenced using macros. The first occurrence is automatically replaced with the long version of the acronym, while all subsequent usages use the abbreviation.

% E.g. \texttt{\textbackslash ac\{TUM\}, \textbackslash ac\{TUM\}} $\Rightarrow$ \ac{TUM}, \ac{TUM}

% For more details, see the documentation of the \texttt{acronym} package\footnote{\url{https://ctan.org/pkg/acronym}}.
% \subsection{Subsection}

% See~\autoref{tab:sample}, \autoref{fig:sample-drawing}, \autoref{fig:sample-plot}, \autoref{fig:sample-listing}.
Deep learning is a branch of machine learning that uses neural networks to learn from
data and perform tasks such as computer vision, which is the field of study that enables 
machines to understand and analyze visual information such as images and videos.
Computer vision algorithms analyze certain criteria in images and videos, and then
apply interpretations to predictive or decision-making tasks. Most recent computer 
vision algorithms are deep learning based, as they can leverage the power of multiple
layers of linear and non-linear operations to extract complex features from the input
data. Deep learning algorithms use back propagation and weight optimization to adjust
the network parameters based on the error between the predicted and actual outputs~\cite{testwebsite}.
However, deep learning and computer vision are not deterministic fields~\cite{zhuang2022randomness}. There are
various sources of randomness and irreproducibility that can affect the performance 
and reliability of deep learning models and computer vision algorithms.
\\
\\
Randomness is in a direct relation with the reproducibility. It introduces an element of unpredictability into the whole training process
of neural network. When randomness is involved, it means that the same input
conditions can lead to different outcomes as results. Deliberate investigation
of this randomness is needed to ensure reproducibility in machine learning. In the thesis, about reproducibility,
we adopt the meaning used by Goodman et al.~\cite{goodman:2016ca} who says it is the ability to obtain the exact same results, by implementing procedures using the same data
and tools. Reproducibility is a cornerstone of the scientific method, and it is essential for ensuring that the results
of computer vision tasks are valid and can be trusted so that the results could be used when giving decisions
about a patient or reliability of a construction site. Reproducibility can also help to minimize the amount of time and
resources required for computer vision by reducing the need for additional experimentation to confirm
results.
\\
\\
Reproducibility issues in computer vision can have negative consequences, 
such as unreliable results and reduced confidence in research findings. 
To ensure that new methods are credible, researchers should share relevant resources such as datasets, 
trained models, training parameters, and evaluation scripts in their publications. 
This enables other researchers to replicate the experiments and confirm the results, 
which can contribute to advancements in the field. Despite these efforts, reproducing 
the results may still pose challenges due to various factors.~\cite{chenetal}.
\\
\\
There are several sources of randomness and irreproducibility. Randomness is a source of irreproducibility.
We focus on randomness in deep neural networks as a source of irreproducibility. There are implementation level
randomness and algorithmic level randomness~\cite{zhuang2022randomness}. In deep learning algorithms, some of this randomness is introduced to
prevent algorithm to memorize the data. However, as stated above, there are obvious drawbacks of the randomness. In this study we aim to use three datasets and make experiments on these
datasets. In each dataset, we will control deterministic CUDA execution and by fixing other factors we look at the
trade offs. Namely, we investigate the randomness caused by the CUDA~\cite{ghorpade2012gpgpu} execution. We, then, investigate how the experiments performed out
in the three datasets we are using. It is expected that at each different settings 
performance will be different. The trade-off between task performance, runtime, and computation costs will be analyzed.
Although it is not one of the main goals of this study, we plan to investigate the randomness introduced by the distributed settings as well.

\section{Section}

\begin{table}[htpb]
  \caption[Example table]{An example for a simple table.}\label{tab:sample}
  \centering
  \begin{tabular}{l l l l}
    \toprule
      A & B & C & D \\
    \midrule
      1 & 2 & 1 & 2 \\
      2 & 3 & 2 & 3 \\
    \bottomrule
  \end{tabular}
\end{table}



\begin{figure}[htpb]
  \centering
  % This should probably go into a file in figures/
  \begin{tikzpicture}[node distance=3cm]
    \node (R0) {$R_1$};
    \node (R1) [right of=R0] {$R_2$};
    \node (R2) [below of=R1] {$R_4$};
    \node (R3) [below of=R0] {$R_3$};
    \node (R4) [right of=R1] {$R_5$};

    \path[every node]
      (R0) edge (R1)
      (R0) edge (R3)
      (R3) edge (R2)
      (R2) edge (R1)
      (R1) edge (R4);
  \end{tikzpicture}
  \caption[Example drawing]{An example for a simple drawing.}\label{fig:sample-drawing}
\end{figure}

\begin{figure}[htpb]
  \centering

  \pgfplotstableset{col sep=&, row sep=\\}
  % This should probably go into a file in data/
  \pgfplotstableread{
    a & b    \\
    1 & 1000 \\
    2 & 1500 \\
    3 & 1600 \\
  }\exampleA
  \pgfplotstableread{
    a & b    \\
    1 & 1200 \\
    2 & 800 \\
    3 & 1400 \\
  }\exampleB
  % This should probably go into a file in figures/
  \begin{tikzpicture}
    \begin{axis}[
        ymin=0,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=Y,
        xlabel=X
      ]
      \addplot table[x=a, y=b]{\exampleA};
      \addlegendentry{Example A};
      \addplot table[x=a, y=b]{\exampleB};
      \addlegendentry{Example B};
    \end{axis}
  \end{tikzpicture}
  \caption[Example plot]{An example for a simple plot.}\label{fig:sample-plot}
\end{figure}

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=SQL]
    SELECT * FROM tbl WHERE tbl.str = "str"
  \end{lstlisting}
  \end{tabular}
  \caption[Example listing]{An example for a source code listing.}\label{fig:sample-listing}
\end{figure}

