% !TeX root = ../main.tex
In this chapter, we discuss the implications of our findings, comparing them with existing literature and interpreting their significance in the broader context of safety in AI. The results presented in Chapter 5 provide insights into CUDA-randomness, laying a foundation for further research and potential applications. Additionally, the analysis in Chapter 6 on environmental impact extends the scope of our discussion beyond the technical domain, highlighting the broader implications of our work. As we progress through this discussion, references to visual representations from the preceding chapters will be made to enhance the interpretation and understanding of our results. This discussion aims to provide a deeper understanding of our empirical findings, linking them to the larger narrative of reproducibility and sustainability in AI, and thereby underlining the multi-dimensional significance of this study.\\

In conclusion, we would like to point out that we try to provide some guidelines or suggestions for future research and applications in the field of AI safety. Drawing from our empirical findings and the existing literature, we propose a set of best practices and recommendations that can be adopted by researchers, developers, and policymakers alike. These guidelines not only address the technical challenges identified in our study but also emphasize the ethical and environmental considerations that are crucial for the sustainable development of AI technologies. By integrating these insights into the design, implementation, and evaluation of AI systems, we believe that the AI community can move towards a more responsible and informed approach to innovation. In the subsequent sections, we will delve into these guidelines in detail, providing a roadmap for ensuring that the advancements in AI are both reproducible and sustainable.
\section{Interpretation of Results}

\subsection{Aggregated Results}
In the chapter 5, we presented the results from total 180 runs for each configurations
along with the difference from the mean for all the datasets. We can how the specific configurations
performed on each dataset and how different seeds and optimizers affected the performance of the model and influenced 
by the CUDA-randomness. Apart from the observations made in the chapter 5, we can discuss the 
outcome of the results in the following manner.\\

Table \ref{tab:cifar10_updated} provides an in-depth evaluation of performance metrics on the CIFAR-10 dataset. The SGD optimizer, when initialized with seed 0 in deterministic mode, stands out by achieving an accuracy of 94.96\%. This deterministic behavior implies that the model, under these conditions, consistently predicts the exact same 94.96\% subset of the 10,000 test images across all runs. Conversely, the ADAM optimizer's performance is less consistent. Specifically, when initialized with seed 314 in deterministic mode, it registers the lowest accuracy of 92.17\%. Interestingly, the non-deterministic variant of this configuration trails by a mere 0.01\%. Chapter 5 had previously highlighted the suboptimal performance of the ADAM optimizer on the CIFAR-10 dataset when compared to SGD.\\

The difference from the non-deterministic mean offers further insights. A notable deviation of -0.910\% is observed for the ADAM optimizer with seed 180698, suggesting that its non-deterministic mode has a slight edge, potentially benefiting from inherent randomness. In comparison, the SGD optimizer's maximum deviation is a modest 0.194\%. This data underscores the consistent performance of the SGD optimizer and the relative variability of ADAM, emphasizing the significance of optimizer selection and configuration for achieving optimal results on specific datasets.\\

In Table \ref{tab:sdnet_updated}, the highest accuracy is observed when using the ADAM optimizer with a seed value of 180698 in deterministic mode, achieving an F1-score of 0.9382. In contrast, the lowest score recorded is 0.9158, which is associated with the SGD optimizer and a seed value of 314. Notably, one configuration displayed a significant performance difference, nearly 1\%, between its deterministic and non-deterministic modes. On aggregate, there is an equal number of instances where deterministic configurations outperformed or underperformed their non-deterministic counterparts. However, this variance might be attributed to the inherent benefits of randomness.\\

For the mammography task results presented in Table \ref{tab:cbis-ddsm_updated}, the disparities between deterministic and non-deterministic modes are more pronounced compared to the other two datasets. In the deterministic setting, the ADAM optimizer with a seed value of 42 yielded the second-highest performance, ensuring consistent results. However, the mean performance over five runs with seed 3407 might be lower if additional runs were conducted. The most significant performance gap between the two modes, approximately 2\%, was observed with seed 314 using the ADAM optimizer in deterministic mode with CUDA execution. This difference is deemed substantial within the computer vision community. Intriguingly, the deterministic configuration outperformed the non-deterministic mean by approximately 2\%.\\

Overall, while the aggregated results provide a broad overview of the performance metrics across different configurations, they do not offer granular insights into the nuances of each configuration's behavior. We have presented and briefly discussed these aggregated outcomes to provide an initial understanding. However, it is essential to delve deeper into the trade-offs associated with each configuration, which we will explore in the subsequent subsections. From our preliminary observations, it is evident that individual configurations exhibit optimal performance on specific datasets. Furthermore, the efficacy of the chosen hyperparameters for a given optimizer cannot be understated. Their selection and tuning play a pivotal role in determining the overall performance, a topic we will address in more detail later in this chapter.

\subsection{Variations in Performance}

In this section, we will discuss the variations in performance observed across different configurations. The visualizations are presented for each configuration and for each configuration we reported the two different metrics for the non-deterministic results to test whether different metrics would cause different variability in the performance with CUDA randomness is being free.\\

Starting with the figure \ref{fig:cifar10_var}, the obvious interpretation is that ADAM optimizer introduces 2 to 3 times more variance than the SGD optimizer indicating the less stability in ADAM optimizer in this case. We observe almost zero difference in variance between the two different metric. Additionall, our findings indicate that seed value 314 introduced more variance than others seeds for both optimizer pointing out that regardless of optimizer and hyperparameter selection some specific seeds may produce more variance than others.\\

In the foundational paper~\cite{zhuang2022randomness}, standard variances of 0.001 are observed for the Cifar-10 Dataset when using Resnet18 on an RTX5000. It's noteworthy that they report their results in terms of implementation-level randomness, which primarily encompasses CUDA randomness. In contrast, our study, conducted on an RTX A6000, focuses exclusively on reporting the CUDA randomness. The validity of this comparison is somewhat constrained due to a slight difference in model selection (PreActResnet18). Nevertheless, the best configuration in our study, in terms of standard variances, aligns closely with the results from the base paper. All other variances we observed are higher, with some instances being up to five times greater. At this point, it is important to acknowledge that different seed configuration could yield far more different outcomes so that they can not be ignored when reporting the variability in performance.\\

In Figures \ref{fig:cbisddsm_var} and \ref{fig:sdnet_var}, we present the standard deviation of performance metrics across two distinct metrics. For the CBIS-DDSM dataset, it becomes evident that the F1-score is significantly influenced by randomness. This pronounced variance underscores the relative unreliability of the F1-score for the CBIS-DDSM dataset, especially when juxtaposed with the consistent performance of the AUC score. Additionally, the variance observed across metrics may be attributed, in part, to the choice of optimizer.\\

For the SDNET dataset, a similar trend is observed, with the F1-score exhibiting a marginally higher variance.

In summary, from the three figures presented to elucidate the impact of CUDA-randomness on performance variability across three datasets and two optimizers, we can deduce the following:

\begin{itemize}
    \item The effect of the optimizer is contingent upon the specific dataset in question.
    \item Different performance metrics can manifest varying degrees of variability.
    \item The seed value can also introduce variability in performance metrics.
    \item Notably, the performance variance in the mammography task is markedly greater than that observed in the other two datasets.
\end{itemize}

\subsection{Tradeoffs between Deterministic and Non-deterministic Mode}

In this section, we will discuss the tradeoffs in performance observed across different configurations and provide valuable context for our second research subquestion. We investigate the runtime and performance difference. Before running the experimentations, it is expected that deterministic mode would increase higher runtime than non-deterministic mode ~\cite{zhuang2022randomness}~\cite{pytorch_randomness}. To test this hypothesis, we have plotted the runtime of each configuration in the figures \ref{fig:cifar10_dif_run}, \ref{fig:cbisddsm_dif_run} and \ref{fig:cbisddsm_dif_run}. We also plotted the performance differences in the figures \ref{fig:cifar10_dif_per}, \ref{fig:cbisddsm_dif_per} and \ref{fig:sdnet_dif_per} to see whether the performance difference between deterministic and non-deterministic mode is correlated with the runtime difference or whether there are general performance implications.

\subsubsection*{In Performance}

The performance trade-offs are more discernible in the visual representations compared to the aggregated results presented in Tables \ref{tab:cifar10_updated}, \ref{tab:sdnet_updated}, and \ref{tab:cbis-ddsm_updated}. Beyond the initial discussions, it is evident from the visualizations that certain configurations demonstrate more robust and stable performance than others. Specifically, the SGD optimizer initialized with seeds 42 and 0 exhibits minimal performance disparities between deterministic and non-deterministic modes.\\

Upon examining the three figures, there isn't a consistent trend in the performance differences between deterministic and non-deterministic modes. However, for both the CIFAR10 and crack detection datasets, the performance disparities remain within a 1\% margin. Given this observation, employing deterministic settings may be preferable to ensure reproducibility. In contrast, for the mammography task, the performance differences occasionally surpass the 1\% threshold, with the most pronounced difference being 2\% in favor of deterministic algorithms. It's crucial to note that this particular result is contingent upon a specific seed configuration and may not be universally applicable. Consequently, drawing a generalized conclusion about the performance trade-offs between deterministic and non-deterministic modes remains challenging.\\

\subsubsection*{In Runtime}

One potential limitation of employing fully deterministic settings to ensure complete reproducibility is the associated runtime. An extended runtime could significantly hinder the applicability of deterministic operations in wider contexts. In Figures \ref{fig:cifar10_dif_run}, \ref{fig:cbisddsm_dif_run}, and \ref{fig:sdnet_dif_run}, we illustrate the runtime disparities between deterministic and non-deterministic modes for each configuration. The runtime difference is computed by subtracting the non-deterministic mode's runtime from that of the deterministic mode.\\

For the CIFAR-10 dataset, as depicted in Figure \ref{fig:cifar10_dif_run}, the runtime difference is predominantly positive, suggesting that the deterministic mode incurs a delay of up to 7.65\%. However, this observation is not consistent across all datasets. Specifically, for the mammography dataset, six configurations are slower in deterministic mode, while four are faster. Similarly, for the concrete crack detection dataset, five configurations are slower, and five are faster in deterministic mode. It's crucial to highlight that there is no direct correlation between runtime and performance differences. Additionally, the distribution of slower and faster configurations is such that it leads to a singular observation: deterministic execution does not invariably result in prolonged training times for the mammography and concrete crack detection datasets. Naturally, the algorithms, architectures, and techniques employed play a pivotal role in this, as they influence the algorithmic choices made by PyTorch.\\

\subsection{Weights Analysis}
In previous analyses, we explored the variances and differences in performance, leading to several conclusions. It's recognized in deep learning that the performance of models is significantly influenced by their trained weights. Therefore, examining these weights, especially in the context of inherent CUDA-randomness, can provide a deeper insight into reproducibility and further support our findings.\\

In earlier chapters, we presented the variances and similarities observed in the weights and detailed the methods used for their calculation. In this section, we will discuss the implications of our weight analysis in terms of these variances and similarities. The primary objective is to determine the degree to which the weights are impacted by CUDA-randomness during training.

\subsubsection*{The Standard Deviation in the Weights}

The standard deviation of the weights is illustrated in the set of figures \ref{fig:variance_plots}. Our analysis reveals that the impact of different seed configurations varies across datasets and optimizers. For instance, in the mammography task, the variances with the ADAM optimizer are minimal compared to other variances. However, as observed earlier, the performance variances were notably high. This underscores the challenging nature of the task, where minor fluctuations in weights can significantly affect the model's classification capability. Another observation in this task is the proximity of seed configurations to one another, resulting in negligible differences. A plausible explanation for this could be the pretrained weights. Each model initialization uses the same weights, and fixing the seed predominantly affects other sources of randomness.\\

For the other two tasks, a distinct contrast is evident between the behaviors of the ADAM and SGD optimizers. SGD exhibits greater stability, with variances that are consistently lower than those of the ADAM optimizer. The disparities between seed values are also less pronounced. It's worth noting that, in the concrete crack detection task, the performance variability with ADAM was actually lower than with SGD. However, when examining figures \ref{fig:sdnet_adam} and \ref{fig:sdnet_sgd}, making direct comparisons is challenging since different seeds produce varying variances, and the overall range appears somewhat comparable. Nevertheless, it's unequivocal that the SGD optimizer offers greater stability in weight variances and achieves optimization more consistently. This is particularly evident with the CIFAR-10 dataset, where SGD outperforms in all discussed facets, and the variance further diminishes post-convergence. This suggests that the SGD optimizer is more adept at locating the global optimum compared to the ADAM optimizer.

\subsubsection*{The Similarity of the Weights}

The similarity of each configuration is represented by 30 mini-plots, grouped under two main figures (\ref{fig:similarities_sgd} and \ref{fig:similarities_adam}). Each figure displays the maximum, minimum, and mean similarity values for every epoch. Mirroring our variance analysis, our objective is to ascertain the extent to which the weights are influenced by CUDA-randomness during training. In this context, observing how similarity evolves over the course of training is more insightful than merely focusing on similarity ranges and values. This is because different tasks might exhibit vastly different similarity metrics, making intra-task comparisons more meaningful. It's also crucial to recognize that similarities are profoundly affected by hyperparameter choices. These hyperparameters are selected to optimize accuracy, which means that within a given task, we might encounter either slow or rapid optimization, both of which can significantly shape similarity trends.\\

Bearing these factors in mind, we can deduce that SGD consistently exhibits stable curves, characterized by fewer fluctuations and reduced similarities throughout training. Additionally, the disparity between maximum and minimum similarities in specific configurations is narrower with SGD. Even in the mammography task, we notice a trend towards convergence with potential for further training. In contrast, with the ADAM optimizer, despite the network's convergence, the similarity continues to display a declining trend. While the figures don't conclusively pinpoint which seed configuration offers superior similarity trends, they undeniably highlight the varying influence of different seed configurations on the training process.\\

In a broader context, seed configurations introduce variations in similarity, at times leading to disparities between mean and minimum values or unforeseen shifts in the trends. The choice of optimizer notably impacts this seed-induced sensitivity. Overall, SGD tends to produce more stable and smooth curves and introduces less similarity than ADAM. Furthermore, the selection of the dataset plays a significant role in determining the stability and sensitivity of similarity trends across different seed and optimizer combinations.

\subsection{Statistical Tests}

In our investigation, we analyzed the performance of various configurations, each determined by a unique combination of seed and optimizer. For each such configuration, we executed five runs incorporating randomness and compared these to a deterministic counterpart. Our goal was to ascertain if a specific seed-optimizer combination or more generally one particular optimizer results in performance that significantly diverges from its deterministic behavior or its counterpart.

\subsection*{T-test with Population Mean}

The one-sample t-test, also known as the t-test with population mean, is a statistical procedure employed to determine if a sample of observations could have been generated by a process with a specific mean. In the context of our investigation, we utilize this test to ascertain if the performance of a specific seed-optimizer combination significantly diverges from its deterministic counterpart. To ensure the validity and robustness of this test, it's crucial to understand and satisfy its underlying assumptions.

\begin{itemize}
    \item \textbf{Independence of Observations}: One of the foundational assumptions of the t-test is the independence of observations. In our deep learning experiments, this assumption implies that the outcome of one run of the model should not influence the outcome of another. Given the inherent and uncontrollable randomness introduced by GPU operations, especially the hardware scheduler and floating-point operations, each run's outcome is independent of any other run. This ensures that the results from different runs, even under identical conditions, are not influenced by prior runs.

    \item \textbf{Normality}: The t-test assumes that the differences between the sample means and the population mean are approximately normally distributed. Given the multitude of minute effects influencing each run, such as individual weight initializations, atomic operations, and floating-point arithmetic, the Central Limit Theorem suggests that our results would be approximately normally distributed. However, it's always prudent to conduct preliminary evaluations, such as Q-Q plots, to verify this assumption.

    \item \textbf{Ratio Scale of Measurement}: The accuracies derived from our machine learning models operate on a ratio scale. This means that an accuracy of 0\% signifies the complete absence of correct predictions, and the intervals between accuracy values are consistent and meaningful. As the t-test requires data that supports meaningful mathematical operations and comparisons, our accuracy measurements, being on a ratio scale, fit the requirements of the t-test.
\end{itemize}

It's also important to recognize the distinctiveness of each seed-optimizer combination. In statistical terms, while tests across configurations are independent, they also remain conceptually autonomous. Each combination delineates a separate, individual scenario we aimed to explore.\\

In many statistical analyses, when multiple tests are conducted simultaneously, there arises a concern known as the multiple comparison problem. This problem highlights the increased risk of observing a statistically significant result purely by chance, especially as the number of tests increases. Traditionally, to counteract this risk, corrections like the Bonferroni correction are applied. The Bonferroni method adjusts the significance level based on the number of tests, making the criteria for significance stricter and thereby reducing the chance of false positives.\\

However, given our experimental structure, these traditional concerns associated with the multiple comparison problem don't apply in the typical manner. Unlike many studies where results from different tests are aggregated to claim a single significant outcome or where multiple tests are used to reinforce a singular hypothesis, our approach is distinct. In our investigation, each test is treated as an independent entity, standing on its own merit. Rather than making broad claims, each test specifically addresses a unique question related to its corresponding seed-optimizer pairing. As such, the inherent independence of each test reduces the need for traditional multiple comparison corrections.\\

Furthermore, the practical ramifications of a Type I error (incorrectly rejecting the null hypothesis) in our context are moderate. Each conclusion drawn about a configuration is insulated from the others, ensuring that a potential error in one test doesn't cascade or amplify implications across the entire study.\\

Thus, considering the conceptual independence of tests, the satisfied assumptions, and the isolated implications of potential errors, we argue that the application of the t-test is appropriate for our scenario. Additionally, the Bonferroni correction or similar multiple comparison adjustments would be overly conservative and not scientifically warranted in our particular setup.\\

In the light of these considerations, in the table \ref{tab:stat_results} we present the results of the t-test with population mean for each configuration. 6 p-values out of 10 for the non-deterministic runs that belong the mammography task shows the results are statistically significant indicating that mammography task is heavily influenced by the CUDA-randomness. 
Overall, table indicates a distinct challenges accross different domains indicating the delicate analyses of randomness within the domains and even subdomains. Further analysing the table, we see the huge jumps in the p-values with different seed values. This means that seed value selection might be crucial for the reproducibility of the results and credibility of the findings.\\

\subsection*{ANOVA and Kruskal-Wallis Test}

In the same manner, we also conducted ANOVA and Kruskal-Wallis tests to determine if the performance of a specific optimizer significantly diverges from its counterpart. The same assumptions we made previously could be made in this case as well. The results of these tests are presented in the table \ref{tab:anova_kruskal_results}. We also presented the F-values which is the ratio of the variance between groups to the variance within groups. The F-value is used to determine the p-value which is not presented in the t-test due to space limitations. But the important metric here is the p-value. From the table, we see the p-values with the ADAM optimizer on mammography and CIFAR-10 tasks indicate the statisticaly significant results. Pointing out again that the ADAM optimizer is more
sensitive to the CUDA-randomness than the SGD optimizer. For the concrete crack detection, the p-values with SGD optimizer indicates the statistically significant result but the p-values for ADAM are still close the threshold.\\

\section{Implications of CUDA-Induced Randomness on Reproducibility}
The results of our investigations carry significant implications for the broader deep learning community. Our work aims to illuminate the often-overlooked influence of CUDA-induced randomness on the reproducibility of deep learning experiments. Notably, even in controlled environments, we observed varying sensitivities to CUDA-randomness across different seeds, optimizers, and datasets. Contrary to common belief, our findings indicate that fully deterministic settings don't always compromise performance or increase overhead. In some instances, deterministic configurations outperformed their non-deterministic counterparts, potentially due to the adverse effects of randomness. This phenomenon might be attributed to advancements in hardware, software updates, and sophisticated algorithms and operations introduced by platforms like PyTorch and Nvidia.

To foster a culture of reproducibility and robustness in deep learning research, we offer the following recommendations:

\begin{enumerate}
    \item \textbf{Embrace Deterministic Settings}: Researchers, practitioners, and developers should consider experimenting with fully deterministic settings in their work. The potential benefits, as our findings suggest, could be substantial.
    \item \textbf{Experiment with Multiple Seeds}: To understand the nuances of specific seed configurations, it's crucial to conduct experiments with a variety of seed values.
    \item \textbf{Full Disclosure of Experimental Settings}: For the sake of reproducibility, always report all settings, configurations, and versions used in experiments. This transparency ensures that others can replicate and validate findings.
    \item \textbf{Stay Updated}: With the rapid evolution of hardware and software, it's essential to stay updated. Newer versions might introduce changes that affect reproducibility.
    \item \textbf{Step-by-Step Documentation}: Detailed documentation of every step in the experimental process can significantly aid reproducibility. This includes documenting data preprocessing steps, model architectures, training procedures, and evaluation metrics.
    \item \textbf{Open Source Your Work}: Making your code, datasets, and models available to the public can greatly enhance reproducibility. Open-sourcing not only allows others to replicate your results but also fosters collaboration and accelerates research.
\end{enumerate}

\section{Limitations and Future Work}

Our study, while offering valuable insights into the challenges of reproducibility in deep learning, has certain inherent limitations. Each limitation also presents an opportunity for further research, allowing the community to delve deeper into the nuances of reproducibility and CUDA-induced randomness.

\begin{itemize}
    \item \textbf{Scope of Experiments}: The constraints of this being a master's thesis meant that the number of experiments for each configuration was limited to five. Future research with a more extensive set of runs would provide a more comprehensive understanding and potentially more robust findings.
    
    \item \textbf{Dataset Selection}: The datasets employed in this study were chosen for their representativeness and feasibility. Exploring more extensive datasets like ImageNet, CIFAR-100, or the recently introduced RSNA dataset in future studies might offer different perspectives. The RSNA dataset, as highlighted by Jafari et al.~\cite{jafari2023breast}, shows promise but requires a thorough reproducibility analysis.
    
    \item \textbf{Hyperparameter Selection}: While our hyperparameters were informed by prior studies, a more exhaustive search could be beneficial. The performance of different optimizers can vary based on hyperparameters and can exhibit dataset-specific sensitivities, making this a ripe area for future exploration.
    
    \item \textbf{Related Work on CUDA-Induced Randomness}: The focus of our study on CUDA-induced randomness and reproducibility has shown that there is limited directly-related work in this specific area. This gap indicates a need for further research. Future studies should address the theoretical challenges presented by the CUDA architecture, especially concerning parallel operations and their impact on computation consistency. It might be beneficial to develop mathematical methods to reduce the randomness in computations. Additionally, researching and implementing optimization techniques that have a reduced sensitivity to parallel operations in GPUs could lead to more consistent results in deep learning experiments.
    
    \item \textbf{Architectural Variations}: Different neural network architectures can introduce variability in results. Our study did not explore this potential source of variability, which means different architectures might exhibit distinct sensitivities to CUDA randomness.
    
    \item \textbf{Distributed Settings}: Our non-distributed experimental setup leaves room for investigating the interplay of distributed deep learning with CUDA-randomness, especially as distributed methodologies gain traction.
    
    \item \textbf{Framework-Specific Analysis}: While centered around PyTorch, other frameworks like TensorFlow might have different reproducibility characteristics. A cross-framework analysis, considering the nuances of isolating CUDA-randomness, can offer a broader view of the reproducibility landscape.
\end{itemize}


Our findings shed light on the difficult challenges of ensuring reproducibility in deep learning experiments, emphasizing the subtleties introduced by CUDA randomness. These identified limitations not only highlight areas of improvement for our study but also pave the way for future research directions. We hope this study serves as a catalyst for the research community, underscoring the need for meticulous research methodologies and emphasizing the opportunities that lie in addressing these challenges. The goal is to ensure the robustness and credibility of future deep learning research.
